# -*- coding: utf-8 -*-
"""projet_ia_anglais_scribmaps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFINBZay9v5gepcE297G1RA4Dx97JgOX

# Exploration du jeu de donn√©es et nettoyage du dataset
"""

# Importation de pandas
import pandas as pd

!pip install langdetect

from langdetect import detect
import requests
from geopandas import GeoDataFrame

"""## France üá´üá∑"""

# Lecture du CSV avec pandas
df = pd.read_csv("adresses-75.csv", delimiter=";")

# Affichage du data frame
df

# Affichage des dimensions
df.shape

# Affichez le nombre de valeurs manquantes
df.isna().sum()

"""**Dataset** choisi pour la France : https://adresse.data:.gouv.fr/data/ban/adresses/latest/csv"""

# Afficher la nature des donn√©es
df.info()

# Supprimer les colonnes non-utilis√©es
df_france = df.drop(['id', 'id_fantoir', 'code_insee', 'code_insee_ancienne_commune', 'nom_ancienne_commune', 'x', 'y', 'lat', 'lon', 'type_position', 'alias', 'nom_ld', 'libelle_acheminement',  'nom_afnor', 'source_position', 'source_nom_voie', 'certification_commune', 'cad_parcelles'], axis=1)

df_france

# Supprimez les donn√©es dupliqu√©es
df_france = df_france.drop_duplicates()

df_france

# Ajout du pays dans l'adresse
df_france = df_france.assign(pays="France")

df_france

# Essai de la d√©tection de la langue sur une rue
print(detect(df_france.values[0][2]))

df_france.to_csv('data_france.txt', sep='\t', index=False)

with open(r'data_france.txt', 'r') as file:
  data = file.read()
  data = data.replace("		", " ")

with open(r'data_france.txt', 'w') as file:
    file.write(data)

import gensim
import spacy

!python -m spacy download fr_core_news_sm

nlp = spacy.load("fr_core_news_sm")  # Download the French model

# Initialisation du mod√®le Word2Vec
model = gensim.models.Word2Vec(min_count=1, window=5, sg=1)

# Tokenisation
tokenized_data = list()

with open(r'data_france.txt', 'r') as file:
  data = file.read()

lowercase = data.lower()

lines = lowercase.splitlines()

for line in lines:
  tokens = nlp(line)
  #mots = [token.text for token in tokens]
  mots = [token.text for token in tokens if token.text != '\t']
  tokenized_data.append(mots)

tokenized_data

model.build_vocab(tokenized_data)

model.train(tokenized_data, total_examples=model.corpus_count, epochs=10)

word = model.wv.index_to_key[1120]  # Access the word at index 10 (example)
print(word)

mot = "boulevard"  # Exemple de mot
similar_words = model.wv.most_similar(mot)
print(similar_words)

model.save("word2vec_france.model")

from gensim.models import Word2Vec
from scipy.spatial.distance import cosine

def predire_adresse(saisie, adresses, model, seuil_similarite=0.90):
    """Pr√©dit l'adresse la plus probable en fonction de la saisie de l'utilisateur.

    Args:
        saisie (str): La saisie de l'utilisateur.
        adresses (list): Liste des adresses tokenis√©es.
        model (Word2Vec): Le mod√®le Word2Vec entra√Æn√©.
        seuil_similarite (float): Seuil de similarit√© pour consid√©rer une adresse comme valide.

    Returns:
        str or None: L'adresse pr√©dite la plus probable, ou None si aucune adresse ne correspond.
    """

    saisie_tokens = saisie.lower().split()  # Tokenisation de la saisie
    print(saisie_tokens)
    meilleure_adresse = None
    meilleure_similarite = 0

    for adresse in adresses:
        if len(adresse) > len(saisie_tokens):  # V√©rifier si l'adresse est plus longue que la saisie
            similarite = model.wv.n_similarity(saisie_tokens, adresse[:len(saisie_tokens)])
            #print(similarite)
            if similarite > meilleure_similarite and similarite >= seuil_similarite:
                meilleure_suite = " ".join(adresse[len(saisie_tokens):])  # R√©cup√©rer la suite de l'adresse
                print(meilleure_suite)
                meilleure_similarite = similarite
                print(meilleure_similarite)

    return meilleure_adresse

print(predire_adresse("8 rue de", tokenized_data, model))

# Cr√©er un objet DocBin pour stocker les documents spaCy
doc_bin = spacy.tokens.DocBin()

for tokens in tokenized_data:
    doc = spacy.tokens.Doc(nlp.vocab, words=tokens)  # Cr√©er un objet Doc √† partir des tokens
    doc_bin.add(doc)

doc_bin.to_disk("adresses_tokenisees.spacy")

nlp.to_disk("adresses_tokenisees_model.spacy")

nlp = spacy.blank("fr")

ner = nlp.add_pipe("ner")

import re

# 1. Liste des adresses
adresses = [
    "10 Rue de Rivoli\t75001\tParis 1er Arrondissement\tFrance",
    "25 Rue de la Paix\t75002\tParis 2e Arrondissement\tFrance",
    "1 Place Vend√¥me\t75001\tParis 1er Arrondissement\tFrance",
    "8 Avenue Montaigne\t75008\tParis 8e Arrondissement\tFrance",
]

# 2. Initialisation de la liste de donn√©es d'entra√Ænement
TRAIN_DATA = []

# 3. Expressions r√©guli√®res am√©lior√©es
regex_numero = r"^\d+[\w-]*"  # Permet les num√©ros avec lettres ou tirets (ex: 10bis, 10-12)
regex_type_voie = r"\b(all√©e|avenue|boulevard|carrefour|chemin|cours|impasse|place|quai|rue|square|voie|traverse|sentier)\b"
regex_nom_voie = r"(?!\t).*"  # Prend tout ce qui n'est pas une tabulation, apr√®s le num√©ro et le type de voie

# 4. Parcourir les adresses
for adresse in adresses:
    entities = []
    columns = adresse.split("\t")  # Diviser l'adresse en colonnes

    # 5. Extraction des entit√©s avec les expressions r√©guli√®res am√©lior√©es
    if len(columns) >= 3:  # S'assurer qu'il y a au moins 3 colonnes
        numero = columns[0]
        type_voie = columns[1]
        nom_voie = columns[2]

        match_numero = re.search(regex_numero, numero)
        match_type_voie = re.search(regex_type_voie, type_voie, re.IGNORECASE)
        match_nom_voie = re.search(regex_nom_voie, nom_voie, re.IGNORECASE)

        if match_numero:
            entities.append((match_numero.start(), match_numero.end(), "NUMERO"))
        if match_type_voie:
            entities.append((match_type_voie.start(), match_type_voie.end(), "TYPE_VOIE"))
        if match_nom_voie:
            entities.append((match_nom_voie.start(), match_nom_voie.end(), "NOM_VOIE"))

        TRAIN_DATA.append((adresse, {"entities": entities}))

# 6. Affichage des donn√©es d'entra√Ænement
print(TRAIN_DATA)

for _, annotations in TRAIN_DATA:
    for ent in annotations.get("entities"):
        ner.add_label(ent[2])

with nlp.disable_pipes(*unaffected_pipes):
    optimizer = nlp.begin_training()
    for iteration in range(20):
        losses = {}
        for text, annotations in TRAIN_DATA:
            nlp.update(
                [text],
                [annotations],
                drop=0.5,
                sgd=optimizer,
                losses=losses)
        print(losses)